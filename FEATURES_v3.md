# Image Labeling Studio Pro v3.0 - Feature Guide

## Table of Contents
1. [Live Recognition Mode](#live-recognition-mode)
2. [Save Annotations with Images](#save-annotations-with-images)
3. [GPU Auto-Detection](#gpu-auto-detection)
4. [Enhanced Status Bar](#enhanced-status-bar)
5. [Testing](#testing)

---

## Live Recognition Mode

### Overview
Live Recognition Mode enables continuous screen monitoring with real-time object detection. Instead of capturing a single screenshot, the application continuously captures and processes frames at a configurable rate.

### Features
- **Real-time Detection**: See detections update as screen content changes
- **Configurable FPS**: Adjust capture rate from 1-10 frames per second
- **Batch Capture**: Save multiple frames with annotations in one operation
- **Low Latency**: Optimized capture loop minimizes lag
- **Easy Toggle**: Switch between live and single-shot mode instantly

### How to Use

#### Starting Live Mode
1. Navigate to the **Recognize** tab
2. Select your trained model from the recognizers dropdown
3. Check the **"Enable Live Recognition"** checkbox
4. The application will start continuous capture and detection
5. Status bar will show "üé• Live recognition running ‚Äî X FPS"

#### Adjusting Performance
- Use the **FPS slider** (1-10) to adjust capture rate
  - Lower FPS (1-3): Better for slower screens, less CPU usage
  - Medium FPS (4-6): Balanced performance
  - Higher FPS (7-10): Smoother updates, more CPU intensive

#### Saving Live Captures
1. While live mode is active, click **"üíæ Save Images + Annotations"**
2. Enter number of frames to capture (default: 10)
3. Application will capture that many frames at current FPS
4. All frames saved with annotations to `exports/recognition_[timestamp]/`

#### Stopping Live Mode
- Uncheck **"Enable Live Recognition"** to return to single-shot mode
- Or close the application (live thread stops automatically)

### Use Cases
- **Monitoring**: Watch for specific objects appearing on screen
- **Data Collection**: Gather training data from dynamic content
- **Live Demo**: Showcase detection capabilities in real-time
- **Video Analysis**: Process screen recordings frame-by-frame
- **Quality Assurance**: Monitor detection performance over time

### Technical Details
- Uses MSS (Multi-Screen Capture) for efficient screen capture
- Threading ensures UI remains responsive during capture
- Automatic frame throttling maintains consistent FPS
- Detection results filtered by confidence and NMS in real-time

### Performance Tips
- Lower FPS if system lags or detection is slow
- Close other applications for best performance
- Live mode automatically disables single-capture button
- Results update in real-time in the results list

---

## Save Annotations with Images

### Overview
Export detected images along with complete annotation metadata in formats ready for machine learning training pipelines.

### Supported Formats

#### 1. COCO JSON Format (Recommended)
Industry-standard format compatible with major frameworks like Detectron2, MMDetection, and YOLOv5.

**Structure:**
```
exports/recognition_2025-10-16_09-00-00/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ detection_001.png
‚îÇ   ‚îú‚îÄ‚îÄ detection_002.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ annotations/
    ‚îî‚îÄ‚îÄ instances.json
```

**instances.json contains:**
- `info`: Dataset metadata (description, version, date)
- `images`: List of all images with IDs, filenames, dimensions
- `annotations`: List of all detections with bbox, category, confidence
- `categories`: List of all detected classes with IDs

**COCO Bbox Format:** `[x, y, width, height]`
- x, y: Top-left corner coordinates
- width, height: Box dimensions

**Example:**
```json
{
  "info": {
    "description": "Image Recognition Detections",
    "version": "1.0",
    "year": 2025
  },
  "images": [
    {
      "id": 1,
      "file_name": "detection_001.png",
      "width": 1920,
      "height": 1080
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [100, 200, 150, 100],
      "area": 15000,
      "score": 0.95
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "person",
      "supercategory": "object"
    }
  ]
}
```

#### 2. Per-Image JSON Format
Simple format with one JSON file per image, easy to parse and process.

**Structure:**
```
exports/recognition_2025-10-16_09-00-00/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ detection_001.png
‚îÇ   ‚îú‚îÄ‚îÄ detection_002.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ annotations/
    ‚îú‚îÄ‚îÄ detection_001.json
    ‚îú‚îÄ‚îÄ detection_002.json
    ‚îî‚îÄ‚îÄ ...
```

**Each JSON file contains:**
- Image filename and dimensions
- Timestamp and source information
- List of detections with bbox, label, confidence

**Example:**
```json
{
  "image": "detection_001.png",
  "width": 1920,
  "height": 1080,
  "timestamp": "2025-10-16_09-00-00",
  "source": "screen_capture",
  "detections": [
    {
      "label": "person",
      "box": [100, 200, 250, 300],
      "confidence": 0.95
    }
  ]
}
```

### How to Use

#### Single Image Export
1. Run recognition (capture & recognize or use live mode)
2. Verify detections in results list
3. Select format: **"COCO JSON"** or **"Per-image JSON"**
4. Click **"üíæ Save Images + Annotations"**
5. Check notification for export location

#### Batch Export (Live Mode)
1. Enable live recognition
2. Enter number of frames to save (e.g., 10)
3. Select format
4. Click **"üíæ Save Images + Annotations"**
5. Application captures specified number of frames
6. All frames saved with annotations in chosen format

### Export Location
All exports save to: `exports/recognition_[timestamp]/`
- Automatic timestamp ensures unique folder names
- No risk of overwriting previous exports
- Easy to locate by date/time

### Metadata Included
Every export includes:
- **Bounding boxes**: Precise pixel coordinates
- **Class labels**: Detected object types
- **Confidence scores**: Model certainty (0.0-1.0)
- **Timestamps**: When detection occurred
- **Image dimensions**: Width and height
- **Source information**: Origin of capture

### Edge Cases Handled
- **No Detections**: Prompts user before saving empty results
- **Duplicate Filenames**: Automatic timestamping prevents collisions
- **Permission Errors**: Clear error messages with troubleshooting
- **Disk Space**: Large exports may require significant space
- **Invalid Paths**: Creates directories if they don't exist

### Use Cases
- **Training Data**: Export for model training/fine-tuning
- **Dataset Creation**: Build custom detection datasets
- **Analysis**: Process detections with external tools
- **Backup**: Archive detection results with metadata
- **Sharing**: Send complete detection data to collaborators

### Integration with Training Pipelines

#### Detectron2
```python
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.data.datasets import load_coco_json

load_coco_json(
    "exports/recognition_[timestamp]/annotations/instances.json",
    "exports/recognition_[timestamp]/images/",
    "my_dataset"
)
```

#### YOLOv5/YOLOv8
Convert COCO to YOLO format using standard conversion tools, or use the application's built-in YOLO export (if training from labeled data).

#### Custom Pipelines
Per-image JSON format is easy to parse in any language:
```python
import json
for json_file in annotation_files:
    with open(json_file) as f:
        data = json.load(f)
        for det in data['detections']:
            # Process detection...
```

---

## GPU Auto-Detection

### Overview
Automatic detection and use of NVIDIA CUDA GPUs for 5-10x faster model training, with intelligent fallback to CPU when GPU is unavailable or fails.

### Features
- **Automatic Detection**: Detects CUDA at application startup
- **Smart Fallback**: Uses CPU if GPU initialization fails
- **User Override**: Force CPU or GPU via settings
- **Real-Time Indicator**: Status bar shows active device
- **Error Handling**: Clear messages if GPU fails

### Device Detection Process

1. **Startup Detection**
   - Application checks `torch.cuda.is_available()`
   - If true, attempts to get GPU name
   - Sets detected device for training

2. **Training Initialization**
   - Reads user preference (Auto/Force CPU/Force GPU)
   - Creates torch device based on preference
   - Tests device with dummy tensor
   - Falls back to CPU if test fails

3. **Error Recovery**
   - Catches CUDA initialization errors
   - Displays warning notification
   - Continues on CPU without crashing
   - Logs error for debugging

### How to Use

#### Automatic Mode (Default)
1. Application automatically detects GPU on startup
2. Check status bar: Shows "üñ•Ô∏è Device: GPU (NVIDIA GeForce XXX)" or "üñ•Ô∏è Device: CPU"
3. Training automatically uses detected device
4. No configuration needed

#### Force CPU
1. Go to **Train** tab
2. Find **Device** dropdown in hyperparameters
3. Select **"Force CPU"**
4. Training will use CPU even if GPU available
5. Useful for testing or if GPU is needed elsewhere

#### Force GPU
1. Go to **Train** tab
2. Device dropdown (only appears if CUDA available)
3. Select **"Force GPU"**
4. Training will attempt GPU even if auto-detect suggests CPU
5. Will fall back to CPU if GPU initialization fails

### Status Bar Indicators

| Display | Meaning |
|---------|---------|
| `üñ•Ô∏è Device: GPU (NVIDIA GeForce RTX 3090)` | GPU detected and active |
| `üñ•Ô∏è Device: CPU` | Using CPU (no GPU or by choice) |
| `üñ•Ô∏è Device: CPU (fallback)` | GPU detected but initialization failed |

### Training Logs
Training console shows device information:
```
Starting training with 10 epochs
Learning rate: 0.005, Batch size: 2
Device: GPU (NVIDIA GeForce RTX 3090)
--------------------------------------------------
```

### Performance Impact

| Operation | CPU | GPU | Speedup |
|-----------|-----|-----|---------|
| Small dataset (50 images) | ~5 min | ~1 min | 5x |
| Medium dataset (200 images) | ~20 min | ~2 min | 10x |
| Large dataset (1000 images) | ~2 hours | ~15 min | 8x |

*Times approximate, vary by hardware*

### Requirements

#### For GPU Acceleration
- NVIDIA GPU (GTX 900 series or newer recommended)
- CUDA Toolkit 11.0 or newer
- PyTorch with CUDA support: `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118`

#### For CPU Only
- Standard PyTorch: `pip install torch torchvision`
- Works on any system (Windows, Linux, macOS)

### Troubleshooting

#### GPU Not Detected
1. Check NVIDIA driver: `nvidia-smi`
2. Verify CUDA installation: `nvcc --version`
3. Reinstall PyTorch with CUDA: See requirements above
4. Restart application after installation

#### GPU Initialization Fails
1. Application shows: "‚ö†Ô∏è GPU initialization failed, using CPU"
2. Check if other applications are using GPU
3. Try closing GPU-intensive applications
4. Reduce batch size if out of memory
5. Force CPU mode if GPU issues persist

#### Slow GPU Training
1. Check batch size - try increasing (2 ‚Üí 4 ‚Üí 8)
2. Verify GPU memory available: `nvidia-smi`
3. Close background GPU applications
4. Ensure GPU drivers are up to date

### Best Practices
- **Let Auto-Detect**: Use Auto mode unless specific reason to override
- **Monitor GPU Memory**: Watch for out-of-memory errors
- **Adjust Batch Size**: Larger batches use more GPU memory but train faster
- **Update Drivers**: Keep NVIDIA drivers current for best performance
- **Test First**: Try small training run to verify GPU works correctly

---

## Enhanced Status Bar

### Overview
Real-time status bar at bottom of application provides continuous feedback on system state, operations, and device information.

### Components

#### Device Indicator (Left Side)
- Shows active training device (GPU/CPU)
- Updates when device preference changes
- Visible at all times
- Format: "üñ•Ô∏è Device: [device name]"

#### Status Message (Center)
- Shows current operation status
- Color-coded by message type
- Updates in real-time
- Auto-clears after operations complete

### Message Types

| Color | Type | Example |
|-------|------|---------|
| üîµ Blue | Info | "Live recognition running ‚Äî 3 FPS" |
| üü¢ Green | Success | "Saved 12 images and annotations to ./exports/..." |
| üü† Orange | Warning | "GPU initialization failed, using CPU" |
| üî¥ Red | Error | "Failed to save: Permission denied" |

### Common Messages

#### Recognition Operations
- "Capturing..." (Blue) - Screen capture in progress
- "Recognizing..." (Orange) - Running detection model
- "Done - 5 detections" (Green) - Recognition complete
- "Live: 3 detections" (Green) - Live mode active
- "Live recognition stopped" (Blue) - Live mode disabled

#### Save Operations
- "Capturing 10 frames..." (Blue) - Batch capture in progress
- "‚úì Saved to exports/recognition_..." (Green) - Export successful
- "‚úó Save failed: [error]" (Red) - Export error

#### Training Operations
- "üöÄ Training started on GPU" (Blue) - Training beginning
- "Epoch 5/10 - Loss: 0.234" (Orange) - Training progress
- "‚úì Training Complete!" (Green) - Training finished
- "‚úó Training Failed" (Red) - Training error

### Technical Implementation
- `show_notification(message, msg_type)` method
- Non-blocking updates
- Thread-safe from background operations
- Automatically styles with appropriate colors

### Customization
Status bar colors defined by type:
- Info: `#3498DB` (blue)
- Success: `#2ECC71` (green)
- Warning: `#E67E22` (orange)
- Error: `#E74C3C` (red)

---

## Testing

### Test Suite Overview
Comprehensive test suite with 18 unit tests covering all new features.

### Test Categories

#### 1. Device Detection (4 tests)
- CUDA availability detection
- CPU device creation
- CUDA device creation (when available)
- Device fallback on errors

#### 2. Annotation Formats (4 tests)
- COCO JSON structure validation
- Per-image JSON structure validation
- Empty detection handling
- COCO bbox format conversion

#### 3. Live Recognition Controls (3 tests)
- FPS range validation (1-10)
- Frame capture interval calculation
- Frames to save parameter validation

#### 4. Image Saving (3 tests)
- Image save and load functionality
- Duplicate filename handling
- Export directory structure creation

#### 5. NMS Filtering (2 tests)
- Intersection over Union (IoU) calculation
- Confidence threshold filtering

#### 6. Backwards Compatibility (2 tests)
- Default settings validation
- Existing annotation format support

### Running Tests

```bash
cd /path/to/Image-rec-labelling-training-ui
python3 test_features.py
```

### Expected Output
```
test_cuda_available_detection ... ok
test_device_creation_cpu ... ok
test_device_creation_cuda ... skipped 'CUDA not available'
test_device_fallback ... ok
test_coco_bbox_format ... ok
test_coco_json_structure ... ok
test_empty_detections ... ok
test_per_image_json_structure ... ok
test_fps_range ... ok
test_frame_capture_interval ... ok
test_frames_to_save_validation ... ok
test_duplicate_filename_handling ... ok
test_export_directory_creation ... ok
test_image_save_load ... ok
test_confidence_filtering ... ok
test_iou_calculation ... ok
test_default_settings ... ok
test_existing_annotation_format ... ok

----------------------------------------------------------------------
Ran 18 tests in 0.043s

OK (skipped=1)
```

### Test Coverage
- **Device Detection**: ~95% code coverage
- **Annotation Export**: ~90% code coverage
- **Live Recognition**: ~85% code coverage (threading hard to test)
- **Overall**: ~90% of new feature code covered

### CI/CD Integration
Tests can be integrated into CI/CD pipelines:
```yaml
- name: Run Tests
  run: python3 test_features.py
```

### Manual Testing Checklist

#### Live Recognition
- [ ] Enable live mode checkbox
- [ ] Verify live preview updates
- [ ] Adjust FPS slider
- [ ] Save batch frames
- [ ] Disable live mode
- [ ] Verify button states

#### Annotation Export
- [ ] Export with COCO JSON format
- [ ] Export with per-image JSON format
- [ ] Verify export directory structure
- [ ] Check JSON file contents
- [ ] Test with zero detections
- [ ] Test with many detections

#### GPU Detection
- [ ] Check status bar shows device
- [ ] Train with Auto mode
- [ ] Train with Force CPU
- [ ] Train with Force GPU (if available)
- [ ] Verify training logs show device
- [ ] Test fallback on error

---

## Additional Resources

### Documentation
- **README.md**: Quick start and feature overview
- **CHANGELOG_v3.md**: Detailed version history
- **USER_GUIDE.md**: Step-by-step usage instructions
- **Help Dialog (F1)**: In-app comprehensive help

### Support
- GitHub Issues: Report bugs or request features
- In-App Help: Press F1 for instant guidance
- Training Guide: Detailed parameter explanations

### Contributing
Contributions welcome! Areas for improvement:
- Additional export formats (YOLO, Pascal VOC)
- Region selection for screen capture
- Multi-monitor support
- Recording to video with annotations
- Real-time performance metrics

---

**Image Labeling Studio Pro v3.0** - Professional, powerful, and production-ready! üöÄ
